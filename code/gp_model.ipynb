{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in relevant packages\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================================================================================\n",
    "## read in data\n",
    "\n",
    "## winter\n",
    "## collapsed over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_winter_collapsed.csv\", sep=',') \n",
    "## aggregated over time\n",
    "dta = pd.read_csv(\"../data/kcl_london_model_data_winter_agg_time.csv\", sep=',')\n",
    "\n",
    "## not winter\n",
    "## collapsed over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_nowinter_collapsed.csv\", sep=',')\n",
    "## aggregated over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_nowinter_agg_time.csv\", sep=',')\n",
    "\n",
    "## monthly data (2000-2019)\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_monthly.csv\", sep=',')\n",
    "## subset to only one month (1-12)\n",
    "#dta = dta[dta['month']==5]\n",
    "\n",
    "## set variables to use in model\n",
    "params = ['latitude', 'longitude', 'year']\n",
    "#params = ['latitude', 'longitude']\n",
    "ncols= len(params)\n",
    "\n",
    "##==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 7)\n",
      "                            site code   latitude  longitude   site_type  year  \\\n",
      "0               Heathrow Airport  LH2  51.479234  -0.440531  Industrial  2000   \n",
      "1       Barnet - Tally Ho Corner  BN1  51.614675  -0.176607    Kerbside  2000   \n",
      "2         Camden - Swiss Cottage  CD1  51.544219  -0.175284    Kerbside  2000   \n",
      "3  Westminster - Marylebone Road  MY1  51.522540  -0.154590    Kerbside  2000   \n",
      "4              Croydon - Norbury  CR5  51.411349  -0.123110    Kerbside  2000   \n",
      "\n",
      "          nox  \n",
      "0  148.090848  \n",
      "1  199.026427  \n",
      "2  217.054604  \n",
      "3  457.892734  \n",
      "4  235.985324  \n",
      "(1981, 3)\n",
      "(1981, 1)\n",
      "[[148.09084761]\n",
      " [199.0264266 ]\n",
      " [217.05460423]\n",
      " [457.89273426]\n",
      " [235.98532394]\n",
      " [433.92134367]\n",
      " [239.16108482]\n",
      " [167.38472428]\n",
      " [184.71412974]\n",
      " [161.47389498]]\n",
      "[[ 5.16146750e+01 -1.76607000e-01  2.00000000e+03]\n",
      " [ 5.15442190e+01 -1.75284000e-01  2.00000000e+03]\n",
      " [ 5.15225400e+01 -1.54590000e-01  2.00000000e+03]\n",
      " [ 5.14113490e+01 -1.23110000e-01  2.00000000e+03]\n",
      " [ 5.15583462e+01  6.99915095e-02  2.00000000e+03]\n",
      " [ 5.15939350e+01  8.55160000e-02  2.00000000e+03]\n",
      " [ 5.15522640e+01 -4.02779000e-01  2.00000000e+03]\n",
      " [ 5.15107413e+01 -3.71623519e-01  2.00000000e+03]\n",
      " [ 5.14893210e+01 -3.10002000e-01  2.00000000e+03]]\n"
     ]
    }
   ],
   "source": [
    "## preview data\n",
    "print(dta.shape)\n",
    "print(dta.head())\n",
    "\n",
    "## standardize X-values (or should I just subtract 52 from latitude and leave longitude? \n",
    "#if 'year' in dta.columns:\n",
    "#    dta[['year']] = dta[['year']] - np.min(dta[['year']])\n",
    "#if 'latitude' in dta.columns:\n",
    "#    dta[['latitude']] = dta[['latitude']] - 52\n",
    "\n",
    "## divide into features and variable\n",
    "X = dta[params].values\n",
    "y = dta.loc[:,'nox'].values\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "## print previews\n",
    "print(y[0:10])\n",
    "print(X[1:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "feature_scaler = StandardScaler() \n",
    "\n",
    "#if X.shape[1] >= 3:\n",
    "    ## rescale lat/long and year data\n",
    "#    X[:,0:1] = feature_scaler.fit_transform(X[:,0:1])\n",
    "    ## rescale year data from 0 to 1\n",
    "#    X[:,2] = (X[:,2] - min(X[:,2])) / (max(X[:,2]) - min(X[:,2]))\n",
    "#else:\n",
    "\n",
    "## rescale lat/long and year data\n",
    "X = feature_scaler.fit_transform(X)\n",
    "#X[:,0] = X[:,0] - np.mean(X[:,0])\n",
    "#X[:,1] = X[:,1] - np.mean(X[:,1])\n",
    "#X[:,2] = X[:,2] - np.mean(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485, 3)\n",
      "(1485, 1)\n",
      "(496, 3)\n",
      "(496, 1)\n"
     ]
    }
   ],
   "source": [
    "## create validation dataset (no test set since using MLL)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485, 1)\n",
      "(496, 1)\n"
     ]
    }
   ],
   "source": [
    "## Normalize Y (after splitting into training and validation)\n",
    "\n",
    "## standardize y-values\n",
    "y_train = feature_scaler.fit_transform(y_train)\n",
    "y_val = feature_scaler.fit_transform(y_val)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build GP model\n",
    "import gpflow\n",
    "\n",
    "## set kernel\n",
    "## 0: RBF(lat,long)\n",
    "## 1: RBF(lat,long) * RBF(year)\n",
    "## 2: RBF(lat,long) * Linear(year)\n",
    "## 3: RBF(lat,long) * Polynomial(year)\n",
    "which_kernel = 3\n",
    "which_iter = 1\n",
    "\n",
    "if (which_kernel == 1) & ('year' not in params):\n",
    "        print(\"ERROR: did not specify YEAR as parameter!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gpflow.logdensities:Shape of x must be 2D at computation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>prior</th>\n",
       "      <th>transform</th>\n",
       "      <th>trainable</th>\n",
       "      <th>shape</th>\n",
       "      <th>fixed_shape</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPR/kern/kernels/0/lengthscales</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPR/kern/kernels/0/variance</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPR/kern/kernels/1/offset</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPR/kern/kernels/1/variance</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPR/likelihood/variance</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class prior transform  trainable shape  \\\n",
       "GPR/kern/kernels/0/lengthscales  Parameter  None       +ve       True    ()   \n",
       "GPR/kern/kernels/0/variance      Parameter  None       +ve       True    ()   \n",
       "GPR/kern/kernels/1/offset        Parameter  None       +ve       True    ()   \n",
       "GPR/kern/kernels/1/variance      Parameter  None       +ve       True    ()   \n",
       "GPR/likelihood/variance          Parameter  None       +ve       True    ()   \n",
       "\n",
       "                                 fixed_shape value  \n",
       "GPR/kern/kernels/0/lengthscales         True   1.0  \n",
       "GPR/kern/kernels/0/variance             True   1.0  \n",
       "GPR/kern/kernels/1/offset               True   1.0  \n",
       "GPR/kern/kernels/1/variance             True   1.0  \n",
       "GPR/likelihood/variance                 True  0.01  "
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do I need ARD?\n",
    "## \"In our case, we will use the Squared Exponential covariance function for f,\n",
    "## and an Automatic Relevence Determination version of the SE covariance function for g.\n",
    "## The ARD will allow us to find which predictor variables affect predictions from the model,\n",
    "## which should relate to their importance.\"\n",
    "\n",
    "if which_kernel == 0:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1], lengthscales=1.0)\n",
    "if which_kernel == 1:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1], lengthscales=1.0) *\\\n",
    "        gpflow.kernels.RBF(1 , active_dims=[2], lengthscales=0.1)\n",
    "if which_kernel == 2:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1], lengthscales=1.0) *\\\n",
    "        gpflow.kernels.Linear(1 , active_dims=[2])\n",
    "if which_kernel == 3:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1], lengthscales=1.0) *\\\n",
    "                gpflow.kernels.Polynomial(1, degree=2., active_dims=[2])\n",
    "        \n",
    "    \n",
    "## ## option #3) all data\n",
    "## periodic kernel in time (every month) * linear/polynomial (linear trend) * squared exponential in space\n",
    "\n",
    "## Modeling categorical data\n",
    "## There is a simple way to do GP regression over categorical variables.\n",
    "## Simply represent your categorical variable as a by a one-of-k encoding.\n",
    "## This means that if your number ranges from 1 to 5, represent that as 5 different data dimensions,\n",
    "## only one of which is on at a time. \n",
    "## Then, simply put a product of SE kernels on those dimensions.\n",
    "## This is the same as putting one SE ARD kernel on all of them.\n",
    "## The lengthscale hyperparameter will now encode whether, when that coding is active,\n",
    "## the rest of the function changes.\n",
    "## If you notice that the estimated lengthscales for your categorical variables is short,\n",
    "## your model is saying that it's not sharing any information between data of different categories. \n",
    "\n",
    "\n",
    "    \n",
    "## build model\n",
    "m = gpflow.models.GPR(X_train, y_train, kern=kernel)\n",
    "m.likelihood.variance = 0.01\n",
    "\n",
    "## view \n",
    "m.as_pandas_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 735.950639\n",
      "  Number of iterations: 20\n",
      "  Number of functions evaluations: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 735.950639\n",
      "  Number of iterations: 20\n",
      "  Number of functions evaluations: 25\n"
     ]
    }
   ],
   "source": [
    "## Run Model\n",
    "## Marginal Liklihood Maximization\n",
    "## picks the most simple model that picks the data the best\n",
    "gpflow.train.ScipyOptimizer().minimize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     class prior transform  trainable shape  \\\n",
      "GPR/kern/kernels/0/lengthscales  Parameter  None       +ve       True    ()   \n",
      "GPR/kern/kernels/0/variance      Parameter  None       +ve       True    ()   \n",
      "GPR/kern/kernels/1/offset        Parameter  None       +ve       True    ()   \n",
      "GPR/kern/kernels/1/variance      Parameter  None       +ve       True    ()   \n",
      "GPR/likelihood/variance          Parameter  None       +ve       True    ()   \n",
      "\n",
      "                                 fixed_shape                  value  \n",
      "GPR/kern/kernels/0/lengthscales         True  0.0009381454730469734  \n",
      "GPR/kern/kernels/0/variance             True     0.5323843265000349  \n",
      "GPR/kern/kernels/1/offset               True     1.4535385097676954  \n",
      "GPR/kern/kernels/1/variance             True    0.10094021252427628  \n",
      "GPR/likelihood/variance                 True     0.0651536723646352  \n"
     ]
    }
   ],
   "source": [
    "print(m.as_pandas_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score model\n",
    "mean, var = m.predict_y(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "0.9637096774193549\n"
     ]
    }
   ],
   "source": [
    "interval_95 = 2*np.sqrt(var[:,0])\n",
    "interval_95 = np.expand_dims(interval_95, axis=1)\n",
    "top_interval_95 = mean + interval_95\n",
    "bottom_interval_95 = mean - interval_95\n",
    "## evaluate whether point falls in interval\n",
    "truth = []\n",
    "for i in range(y_val.shape[0]):\n",
    "    if y_val[i,0] >= bottom_interval_95[i,0] and y_val[i,0] <= top_interval_95[i,0]:\n",
    "        truth.append(1)\n",
    "    else:\n",
    "        truth.append(0)\n",
    "print(np.sum(truth))\n",
    "score = np.sum(truth) / y_val.shape[0]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06987115996250357\n"
     ]
    }
   ],
   "source": [
    "## also calculate MSE\n",
    "## number of points\n",
    "n = mean.shape[0]\n",
    "diff = (y_val[:,0] - mean[:,0])**2\n",
    "mse = np.sum(diff) / n\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questions for Stefanie:\n",
    "\n",
    "## 1) How to score models? Currently evaluating whether the 95% confidence interval captures the actual value for\n",
    "## the validation set. How to balance the fact that model intervals are too wide?\n",
    "## 2) How to visualize results in 2 and 3-d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_wc_m0 = []\n",
    "\n",
    "mse_wc_m0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Winter\n",
    "score_wc_m0.append(0.9482758620689655)\n",
    "mse_wc_m0.append(0.889188407571395)\n",
    "\n",
    "## Not Winter\n",
    "score_wc_m0.append(0.9482758620689655)\n",
    "mse_wc_m0.append(0.944077087789119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9482758620689655\n",
      "0.9166327476802569\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score_wc_m0))\n",
    "print(np.mean(mse_wc_m0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_w_m0 = []\n",
    "score_w_m1 = []\n",
    "score_w_m2 = []\n",
    "score_w_m3 = []\n",
    "\n",
    "#mse_w_m0 = []\n",
    "mse_w_m1 = []\n",
    "mse_w_m2 = []\n",
    "mse_w_m3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Winter\n",
    "score_w_m1.append(0.9596774193548387)\n",
    "score_w_m2.append(0.9556451612903226)\n",
    "score_w_m3.append(0.9637096774193549)\n",
    "\n",
    "mse_w_m1.append(0.9070767715745317)\n",
    "mse_w_m2.append(0.15715599665719784)\n",
    "mse_w_m3.append(0.06987115996250357)\n",
    "\n",
    "## Non-Winter\n",
    "score_w_m1.append(0.9666666666666667)\n",
    "score_w_m2.append(0.96)\n",
    "score_w_m3.append(0.9555555555555556)\n",
    "\n",
    "mse_w_m1.append(0.8919486748914799)\n",
    "mse_w_m2.append(0.1379321844772651)\n",
    "mse_w_m3.append(0.10993582155549385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631720430107527\n",
      "0.9578225806451612\n",
      "0.9596326164874552\n",
      "0.8995127232330058\n",
      "0.14754409056723147\n",
      "0.0899034907589987\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score_w_m1))\n",
    "print(np.mean(score_w_m2))\n",
    "print(np.mean(score_w_m3))\n",
    "\n",
    "print(np.mean(mse_w_m1))\n",
    "print(np.mean(mse_w_m2))\n",
    "print(np.mean(mse_w_m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_all_m0 = []\n",
    "score_all_m1 = []\n",
    "score_all_m2 = []\n",
    "score_all_m3 = []\n",
    "\n",
    "#mse_all_m0 = []\n",
    "mse_all_m1 = []\n",
    "mse_all_m2 = []\n",
    "mse_all_m3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "## January\n",
    "score_all_m1.append(0.9522727272727273)\n",
    "mse_all_m1.append(0.8456863378328031)\n",
    "\n",
    "score_all_m2.append(0.9295454545454546)\n",
    "mse_all_m2.append(0.293738121763502)\n",
    "\n",
    "score_all_m3.append(0.9318181818181818)\n",
    "mse_all_m3.append(0.28575088342736316)\n",
    "\n",
    "## Febrary\n",
    "score_all_m1.append(0.9501133786848073)\n",
    "mse_all_m1.append(0.9498865601879187)\n",
    "\n",
    "score_all_m2.append(0.9433106575963719)\n",
    "mse_all_m2.append(0.45728805929725613)\n",
    "\n",
    "score_all_m3.append(0.9002267573696145)\n",
    "mse_all_m3.append(1.6687370138120152)\n",
    "\n",
    "## March\n",
    "score_all_m1.append(0.9641255605381166)\n",
    "mse_all_m1.append(0.8622644623424907)\n",
    "\n",
    "score_all_m2.append(0.9551569506726457)\n",
    "mse_all_m2.append(0.9980547846555808)\n",
    "\n",
    "score_all_m3.append(0.9349775784753364)\n",
    "mse_all_m3.append(0.20234739028743054)\n",
    "\n",
    "## April\n",
    "score_all_m1.append(0.9577464788732394)\n",
    "mse_all_m1.append(0.938434528957358)\n",
    "\n",
    "score_all_m2.append(0.9530516431924883)\n",
    "mse_all_m2.append(0.14154699613486757)\n",
    "\n",
    "score_all_m3.append(0.9436619718309859)\n",
    "mse_all_m3.append(0.12071800470906409)\n",
    "\n",
    "## May\n",
    "score_all_m1.append(0.9502369668246445)\n",
    "mse_all_m1.append(0.8891895921499919)\n",
    "\n",
    "score_all_m2.append(0.9597156398104265)\n",
    "mse_all_m2.append(0.1458077967763497)\n",
    "\n",
    "score_all_m3.append(0.9597156398104265)\n",
    "mse_all_m3.append(0.11270673779635344)\n",
    "\n",
    "## June\n",
    "score_all_m1.append(0.9522673031026253)\n",
    "mse_all_m1.append(0.9099270364887586)\n",
    "\n",
    "score_all_m2.append(0.9379474940334129)\n",
    "mse_all_m2.append(0.3122744859095631)\n",
    "\n",
    "score_all_m3.append(0.9212410501193318)\n",
    "mse_all_m3.append(0.2643669979671157)\n",
    "\n",
    "## July\n",
    "score_all_m1.append(0.9527186761229315)\n",
    "mse_all_m1.append(0.9036238395028933)\n",
    "\n",
    "score_all_m2.append(0.9361702127659575)\n",
    "mse_all_m2.append(0.1588594575454301)\n",
    "\n",
    "score_all_m3.append(0.9432624113475178)\n",
    "mse_all_m3.append(0.10461483930076629)\n",
    "\n",
    "## August\n",
    "score_all_m1.append(0.9474940334128878)\n",
    "mse_all_m1.append(0.9413148346599836)\n",
    "\n",
    "score_all_m2.append(0.9474940334128878)\n",
    "mse_all_m2.append(0.19108466579058989)\n",
    "\n",
    "score_all_m3.append(0.9451073985680191)\n",
    "mse_all_m3.append(0.1414502763958607)\n",
    "\n",
    "## September\n",
    "score_all_m1.append(0.9407582938388626)\n",
    "mse_all_m1.append(1.0006019083124036)\n",
    "\n",
    "#score_all_m2.append(np.nan)\n",
    "#mse_all_m2.append(np.nan)\n",
    "\n",
    "score_all_m3.append(0.966824644549763)\n",
    "mse_all_m3.append(0.11038204107636336)\n",
    "\n",
    "## October\n",
    "score_all_m1.append(0.9529411764705882)\n",
    "mse_all_m1.append(0.9437203826510108)\n",
    "\n",
    "score_all_m2.append(0.9458823529411765)\n",
    "mse_all_m2.append(0.2656939430133758)\n",
    "\n",
    "score_all_m3.append(0.9576470588235294)\n",
    "mse_all_m3.append(0.21791853695775756)\n",
    "\n",
    "## November\n",
    "score_all_m1.append(0.9598108747044918)\n",
    "mse_all_m1.append(0.881269262930133)\n",
    "\n",
    "score_all_m2.append(0.9432624113475178)\n",
    "mse_all_m2.append(0.3799254185984513)\n",
    "\n",
    "score_all_m3.append(0.9432624113475178)\n",
    "mse_all_m3.append(0.3481292191320313)\n",
    "\n",
    "## December\n",
    "score_all_m1.append(0.950354609929078)\n",
    "mse_all_m1.append(0.865659369444384)\n",
    "\n",
    "#score_all_m2.append(np.nan)\n",
    "#mse_all_m2.append(np.nan)\n",
    "\n",
    "score_all_m3.append(0.9432624113475178)\n",
    "mse_all_m3.append(0.25517511767401957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525700066479167\n",
      "0.945153685031834\n",
      "0.9409172929506453\n",
      "0.9109648429550107\n",
      "0.3344273729484966\n",
      "0.3193580882113451\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score_all_m1))\n",
    "print(np.mean(score_all_m2))\n",
    "print(np.mean(score_all_m3))\n",
    "\n",
    "print(np.mean(mse_all_m1))\n",
    "print(np.mean(mse_all_m2))\n",
    "print(np.mean(mse_all_m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28575088342736316, 1.6687370138120152, 0.20234739028743054, 0.12071800470906409, 0.11270673779635344, 0.2643669979671157, 0.10461483930076629, 0.1414502763958607, 0.11038204107636336, 0.21791853695775756, 0.3481292191320313, 0.25517511767401957]\n"
     ]
    }
   ],
   "source": [
    "## plot accuracies for each month and model\n",
    "\n",
    "print(mse_all_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
    "plt = matplotlib.pyplot\n",
    "\n",
    "X = np.linspace(-3,3,20)\n",
    "Y = np.random.exponential(np.sin(X)**2)\n",
    "\n",
    "with gpflow.defer_build():\n",
    "    k = gpflow.kernels.Matern32(1, ARD=False) + gpflow.kernels.Bias(1)\n",
    "    l = gpflow.likelihoods.Exponential()\n",
    "    m = gpflow.models.GPMC(X[:,None], Y[:,None], k, l)\n",
    "\n",
    "m.kern.kernels[0].lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kern.kernels[1].lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kernels.Bias.variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "#m.kernels.Matern32.lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kernels.Matern32.variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "#m.kernels.Bias.variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "\n",
    "m.compile()\n",
    "o = gpflow.train.AdamOptimizer(0.01)\n",
    "o.minimize(m, maxiter=15) # start near MAP\n",
    "\n",
    "s = gpflow.train.HMC()\n",
    "samples = s.sample(m, 100, epsilon=0.12, lmax=20, lmin=5, thin=5, logprobs=False)#, verbose=True)\n",
    "samples.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(samples.iloc[:,1]))\n",
    "print(np.var(samples.iloc[:,1]))\n",
    "print(np.mean(samples.iloc[:,2]))\n",
    "print(np.var(samples.iloc[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a more informative plot\n",
    "#plt.figure(figsize=(16, 4))\n",
    "#for lab, s in samples.iteritems():\n",
    "#    plt.plot(s, label=lab)\n",
    "#_ = plt.legend(loc=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for col in samples.columns.sort_values()[1:]:\n",
    "#    samples[col].hist(label=col.split('.')[-1], alpha=0.4, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.linspace(-4,4,100)[:,None]\n",
    "f_samples = []\n",
    "for i, s in samples.iterrows():\n",
    "    m.assign(s)\n",
    "    f_samples.append(m.predict_f_samples(xtest, 5, initialize=False))\n",
    "f_samples = np.vstack(f_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_samples = np.exp(f_samples[:, :, 0])\n",
    "\n",
    "line, = plt.plot(xtest, np.mean(rate_samples, 0), lw=2)\n",
    "plt.fill_between(xtest[:,0],\n",
    "                 np.percentile(rate_samples, 5, axis=0),\n",
    "                 np.percentile(rate_samples, 95, axis=0),\n",
    "                 color=line.get_color(), alpha = 0.2)\n",
    "\n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "plt.ylim(-0.1, np.max(np.percentile(rate_samples, 95, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(N,1)*0.1 + 3\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "\n",
    "k = gpflow.kernels.Matern52(1, lengthscales=0.3)\n",
    "m = gpflow.models.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = 0.01\n",
    "m.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(m):\n",
    "    xx = np.linspace(-0.1, 1.1, 100)[:,None]\n",
    "    mean, var = m.predict_y(xx)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2)\n",
    "    plt.plot(xx, mean, 'b', lw=2)\n",
    "    plt.fill_between(xx[:,0], mean[:,0] - 2*np.sqrt(var[:,0]), mean[:,0] + 2*np.sqrt(var[:,0]), color='blue', alpha=0.2)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as d\n",
    "\n",
    "print(X_train[0:10,:])\n",
    "\n",
    "space_length_scale = 1.0\n",
    "time_length_scale = 1.0\n",
    "\n",
    "#X_space = X_train[:,0:2]\n",
    "\n",
    "X_space = np.atleast_2d(X_train[0,0:2])\n",
    "Y_space = np.atleast_2d(X_train[1,0:2])\n",
    "X_time = np.atleast_2d(X_train[0,2])\n",
    "Y_time = np.atleast_2d(X_train[1,2])\n",
    "print(X_space[0:5,:])\n",
    "print(X_space.shape)\n",
    "print(X_time[0:5,:])\n",
    "print(X_time.shape)\n",
    "\n",
    "\n",
    "dists_space = d.cdist(X_space / space_length_scale, Y_space / space_length_scale, metric='sqeuclidean')\n",
    "dists_time = d.cdist(X_time / time_length_scale, Y_time / time_length_scale, metric='sqeuclidean')\n",
    "K = np.exp(-.5 * dists_space) * np.exp(-.5 * dists_time)\n",
    "\n",
    "print(dists_space)\n",
    "print(dists_time)\n",
    "print(K)\n",
    "\n",
    "\n",
    "dists_space = d.pdist(X_space / space_length_scale, metric='sqeuclidean')\n",
    "dists_time = d.pdist(X_time / time_length_scale, metric='sqeuclidean')\n",
    "K = np.exp(-.5 * dists_space) * np.exp(-.5 * dists_time)\n",
    "# convert from upper-triangular matrix to square matrix\n",
    "K = d.squareform(K)\n",
    "np.fill_diagonal(K, 1)\n",
    "\n",
    "print(dists_space)\n",
    "print(dists_time)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ?? Should I specify a constant kernel?\n",
    "kernel = RBF(length_scale=1)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1, normalize_y=True, n_restarts_optimizer=5)\n",
    "all_accuracies = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2').mean()\n",
    "print(all_accuracies)\n",
    "## winter data\n",
    "## sigma = 1000, alpha=1\n",
    "## 0.8037412498178955\n",
    "## sigma = 1, alpha=1\n",
    "## 0.8055139240920436\n",
    "## sigma=1, alpha = 100\n",
    "## 0.14285243700525588\n",
    "## sigma=10, alpha=100\n",
    "## 0.14285243655664298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best hyperparameters: sigma\n",
    "all_accuracies_mean = []\n",
    "all_accuracies_std = []\n",
    "sigmas = np.arange(start=0.1, stop=4.5, step=0.5)\n",
    "for sigma in sigmas:\n",
    "    kernel = RBF(length_scale=sigma)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1, normalize_y=True, n_restarts_optimizer=5)\n",
    "    all_accuracies = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2')\n",
    "    all_accuracies_mean.append(all_accuracies.mean())\n",
    "    all_accuracies_std.append(all_accuracies.std())\n",
    "print(sigmas)\n",
    "print(all_accuracies_mean)\n",
    "print(all_accuracies_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to select the best parameter\n",
    "def best_parameter (mean_acc, param_list):\n",
    "    ## take sd of all accuracies, if small, then take the median sigma\n",
    "    acc_sd = np.std(mean_acc)\n",
    "    if acc_sd < 0.005:\n",
    "        best_param = np.median(param_list)\n",
    "    else:\n",
    "        ## pick the sigma with the highest R^2\n",
    "        best_param = param_list[np.argmax(mean_acc)]\n",
    "    \n",
    "        ## if large sd and tie, default to larger sigma (more variance)\n",
    "        ## or could select the default value (1)\n",
    "        #best_param = 1\n",
    "\n",
    "    return(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find best parameter for sigma\n",
    "best_sigma = best_parameter(all_accuracies_mean, sigmas)\n",
    "print(best_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best hyperparameters: alpha\n",
    "all_accuracies_al_mean = []\n",
    "all_accuracies_al_std = []\n",
    "alphas = np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "for alpha in alphas:\n",
    "    kernel = RBF(length_scale=best_sigma)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha, normalize_y=True, n_restarts_optimizer=5)\n",
    "    all_accuracies_al = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2')\n",
    "    all_accuracies_al_mean.append(all_accuracies_al.mean())\n",
    "    all_accuracies_al_std.append(all_accuracies_al.std())\n",
    "print(alphas)\n",
    "print(all_accuracies_al_mean)\n",
    "print(all_accuracies_al_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the best alpha\n",
    "best_alpha = best_parameter(all_accuracies_al_mean, alphas)\n",
    "print(best_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
