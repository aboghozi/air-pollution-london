{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in relevant packages\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================================================================================\n",
    "## read in data\n",
    "\n",
    "## winter\n",
    "## collapsed over time\n",
    "dta = pd.read_csv(\"../data/kcl_london_model_data_winter_collapsed.csv\", sep=',') \n",
    "## aggregated over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_winter_agg_time.csv\", sep=',')\n",
    "\n",
    "## not winter\n",
    "## collapsed over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_nowinter_collapsed.csv\", sep=',')\n",
    "## aggregated over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_nowinter_agg_time.csv\", sep=',')\n",
    "\n",
    "## monthly data (2000-2019)\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_monthly.csv\", sep=',')\n",
    "## subset to only January\n",
    "#dta = dta[dta['month']==1]\n",
    "\n",
    "## set variables to use in model\n",
    "#params = ['latitude', 'longitude', 'year']\n",
    "params = ['latitude', 'longitude']\n",
    "ncols= len(params)\n",
    "\n",
    "##==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230, 6)\n",
      "                                   site code   latitude  longitude  \\\n",
      "0                      Heathrow Airport  LH2  51.479234  -0.440531   \n",
      "1  Richmond Upon Thames - Craneford Way  RHC  51.449051  -0.342532   \n",
      "2  Richmond Upon Thames - Craneford Way  RHD  51.449051  -0.342532   \n",
      "3                    Ealing - Horn Lane  EA8  51.518948  -0.265617   \n",
      "4                  Brent - Neasden Lane  BT5  51.552656  -0.248774   \n",
      "\n",
      "    site_type         nox  \n",
      "0  Industrial  142.307206  \n",
      "1  Industrial   72.340567  \n",
      "2  Industrial   71.733144  \n",
      "3  Industrial  135.887743  \n",
      "4  Industrial  130.148268  \n",
      "(230, 2)\n",
      "(230, 1)\n",
      "[[142.30720557]\n",
      " [ 72.34056713]\n",
      " [ 71.7331443 ]\n",
      " [135.88774308]\n",
      " [130.14826753]\n",
      " [103.62288152]\n",
      " [ 61.60730568]\n",
      " [ 56.24558636]\n",
      " [ 89.99836659]\n",
      " [ 65.54394518]]\n",
      "[[51.44905133 -0.3425321 ]\n",
      " [51.44905133 -0.3425321 ]\n",
      " [51.518948   -0.265617  ]\n",
      " [51.552656   -0.248774  ]\n",
      " [51.5526706  -0.24853069]\n",
      " [51.39431543 -0.16516146]\n",
      " [51.39434156 -0.16516104]\n",
      " [51.3892869  -0.14166152]\n",
      " [51.38356523 -0.13641786]]\n"
     ]
    }
   ],
   "source": [
    "## preview data\n",
    "print(dta.shape)\n",
    "print(dta.head())\n",
    "\n",
    "## standardize X-values (or should I just subtract 52 from latitude and leave longitude? \n",
    "#if 'year' in dta.columns:\n",
    "#    dta[['year']] = dta[['year']] - np.min(dta[['year']])\n",
    "#if 'latitude' in dta.columns:\n",
    "#    dta[['latitude']] = dta[['latitude']] - 52\n",
    "\n",
    "## divide into features and variable\n",
    "X = dta[params].values\n",
    "y = dta.loc[:,'nox'].values\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "## print previews\n",
    "print(y[0:10])\n",
    "print(X[1:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 2)\n",
      "(172, 1)\n",
      "(58, 2)\n",
      "(58, 1)\n"
     ]
    }
   ],
   "source": [
    "## create validation dataset (no test set since using MLL)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "#print(max(X_train[:,1]))\n",
    "#print(min(X_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 1)\n",
      "(58, 2)\n",
      "-1.392912629793989\n",
      "4.804710386375748\n"
     ]
    }
   ],
   "source": [
    "## Normalize Y\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "feature_scaler = StandardScaler() \n",
    "\n",
    "## standardize y-values\n",
    "y_train = feature_scaler.fit_transform(y_train)\n",
    "y_val = feature_scaler.fit_transform(y_val)\n",
    "\n",
    "#if X_train.shape[1] >= 3:\n",
    "#    X_train[:,0:2] = feature_scaler.fit_transform(X_train[:,0:2])\n",
    "#    X_val[:,0:2] = feature_scaler.fit_transform(X_val[:,0:2])\n",
    "#else:\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "X_val = feature_scaler.fit_transform(X_val)\n",
    "\n",
    "\n",
    "## TODO: add check for whether columns are correctly standardized, formatted (no strings)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "#print(y_train[0:5,:])\n",
    "#print(X_train.shape)\n",
    "#print(X_train[0:5,:])\n",
    "#print(X_val[0:5,:])\n",
    "print(min(y_train[:,0]))\n",
    "print(max(y_train[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.783487384093839e-16\n",
      "1.0000000000000004\n",
      "1.0145141431919534e-16\n",
      "0.9999999999999999\n",
      "4.5066276292144787e-13\n",
      "1.0\n",
      "7.745742032268534e-17\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_train[:,0]))\n",
    "print(np.var(y_train[:,0]))\n",
    "print(np.mean(y_val[:,0]))\n",
    "print(np.var(y_val[:,0]))\n",
    "\n",
    "print(np.mean(X_train[:,0]))\n",
    "print(np.var(X_train[:,0]))\n",
    "print(np.mean(X_train[:,1]))\n",
    "print(np.var(X_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmc_model_0_iter_1.sav\n"
     ]
    }
   ],
   "source": [
    "## build GP model\n",
    "import gpflow\n",
    "\n",
    "## set kernel\n",
    "## 0: space (1 RBF)\n",
    "## 1: space + time (2 RBF)\n",
    "## 2: space + time (1 RBF, 1 Linear)\n",
    "## 3: space + time + site_type (1 RBF, 1 RBF/Linear, )\n",
    "which_kernel = 0\n",
    "which_iter = 1\n",
    "\n",
    "if (which_kernel == 1) & ('year' not in params):\n",
    "        print(\"ERROR: did not specify YEAR as parameter!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>prior</th>\n",
       "      <th>transform</th>\n",
       "      <th>trainable</th>\n",
       "      <th>shape</th>\n",
       "      <th>fixed_shape</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPMC/V</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>N(0.0,1.0)</td>\n",
       "      <td>(none)</td>\n",
       "      <td>True</td>\n",
       "      <td>(172, 1)</td>\n",
       "      <td>True</td>\n",
       "      <td>[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPMC/kern/lengthscales</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPMC/kern/variance</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPMC/likelihood/scale</th>\n",
       "      <td>Parameter</td>\n",
       "      <td>None</td>\n",
       "      <td>+ve</td>\n",
       "      <td>True</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            class       prior transform  trainable     shape  \\\n",
       "GPMC/V                  Parameter  N(0.0,1.0)    (none)       True  (172, 1)   \n",
       "GPMC/kern/lengthscales  Parameter        None       +ve       True        ()   \n",
       "GPMC/kern/variance      Parameter        None       +ve       True        ()   \n",
       "GPMC/likelihood/scale   Parameter        None       +ve       True        ()   \n",
       "\n",
       "                        fixed_shape  \\\n",
       "GPMC/V                         True   \n",
       "GPMC/kern/lengthscales         True   \n",
       "GPMC/kern/variance             True   \n",
       "GPMC/likelihood/scale          True   \n",
       "\n",
       "                                                                    value  \n",
       "GPMC/V                  [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0....  \n",
       "GPMC/kern/lengthscales                                                1.0  \n",
       "GPMC/kern/variance                                                    1.0  \n",
       "GPMC/likelihood/scale                                                 1.0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do I need ARD?\n",
    "## \"In our case, we will use the Squared Exponential covariance function for f,\n",
    "## and an Automatic Relevence Determination version of the SE covariance function for g.\n",
    "## The ARD will allow us to find which predictor variables affect predictions from the model,\n",
    "## which should relate to their importance.\"\n",
    "\n",
    "if which_kernel == 0:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1])\n",
    "if which_kernel == 1:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1]) * gpflow.kernels.RBF(1 , active_dims=[2], ARD=False)\n",
    "if which_kernel == 2:\n",
    "    kernel = gpflow.kernels.RBF(2, active_dims=[0,1]) * gpflow.kernels.Linear(1 , active_dims=[2], ARD=False)\n",
    "    \n",
    "## build model\n",
    "with gpflow.defer_build():\n",
    "    ## log-liklihood???\n",
    "    l = gpflow.likelihoods.StudentT()\n",
    "    m = gpflow.models.GPMC(X_train, y_train, kernel, likelihood=l)\n",
    "\n",
    "## view \n",
    "m.as_pandas_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set priors for hyperparameters\n",
    "#m.kern.kernels[0].lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kern.kernels[0].variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "#m.kern.kernels[1].lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kern.kernels[1].variance.prior = gpflow.priors.Gamma(1.,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-23 19:27:23.215003\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              GPMC/V  GPMC/kern/lengthscales  \\\n",
      "0  [[0.004937867770740749], [0.2952406052838989],...                0.828747   \n",
      "1  [[0.004937867770740749], [0.2952406052838989],...                0.828747   \n",
      "2  [[0.004937867770740749], [0.2952406052838989],...                0.828747   \n",
      "3  [[0.004937867770740749], [0.2952406052838989],...                0.828747   \n",
      "4  [[0.004937867770740749], [0.2952406052838989],...                0.828747   \n",
      "\n",
      "   GPMC/kern/variance  GPMC/likelihood/scale  \n",
      "0             1.18959               0.821135  \n",
      "1             1.18959               0.821135  \n",
      "2             1.18959               0.821135  \n",
      "3             1.18959               0.821135  \n",
      "4             1.18959               0.821135  \n",
      "2019-04-23 19:31:22.491203\n"
     ]
    }
   ],
   "source": [
    "## Marginal Liklihood Maximization\n",
    "## picks the most simple model that picks the data the best\n",
    "\n",
    "\n",
    "## MODELING\n",
    "## what choice in optimizer??????\n",
    "## settings for optimization\n",
    "## how to evaluate actual model? R^2 \n",
    "## what term do I compare? how to determine \"fit\"?\n",
    "## small value = more complex\n",
    "## should I use 10-fold CV?\n",
    "## should I add a constant kernel?\n",
    "\n",
    "m.compile()\n",
    "opt = gpflow.train.AdamOptimizer(0.01)\n",
    "opt.minimize(m, maxiter=15) # start near MAP\n",
    "\n",
    "## two things to tune\n",
    "## HMC uses Oiler approximation (discrete derivative calculator)\n",
    "## does updates in steps for trajectory of one sample\n",
    "## samples: 100 (how many accepted of the posterior)\n",
    "## epsilon: amount of time in time-steps (dt) -- heuristic??????\n",
    "## lmax, lmin: number of discrete steps along the path -- heuristic???????\n",
    "\n",
    "s = gpflow.train.HMC()\n",
    "sample_df = s.sample(m, 100, epsilon=0.12, lmax=20, lmin=5, thin=5, logprobs=False)\n",
    "print(sample_df.head())\n",
    "\n",
    "## save \n",
    "import pickle\n",
    "## save sample dataframe\n",
    "filename = 'hmc_samples_' + str(which_kernel) + '_iter_' + str(which_iter) + '.sav'\n",
    "pickle.dump(sample_df, open(filename, 'wb'))\n",
    "## save model object\n",
    "filename = 'hmc_model_' + str(which_kernel) + '_iter_' + str(which_iter) + '.sav'\n",
    "#saver= gpflow.saver.Saver()\n",
    "#saver.save(filename, m)\n",
    "pickle.dump(m.read_trainables(), open(filename, 'wb'))\n",
    "\n",
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828746994667614\n",
      "1.1895902366367828\n",
      "0.8211353271974207\n",
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sample_df.iloc[:,1]))\n",
    "#print(np.var(sample_df.iloc[:,1]))\n",
    "print(np.mean(sample_df.iloc[:,2]))\n",
    "#print(np.var(sample_df.iloc[:,2]))\n",
    "print(np.mean(sample_df.iloc[:,3]))\n",
    "#print(np.var(sample_df.iloc[:,3]))\n",
    "#print(np.mean(sample_df.iloc[:,4]))\n",
    "#print(np.var(sample_df.iloc[:,4]))\n",
    "\n",
    "print(sample_df.shape)\n",
    "#print(sample_df[0:5,0])\n",
    "\n",
    "## test points -- x()\n",
    "\n",
    "## make average of line [x1, x2, x3, emissions] -- need to translate the draws compare with \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a more informative plot\n",
    "#plt.figure(figsize=(16, 4))\n",
    "#for lab, s in sample_df.iteritems():\n",
    "#    plt.plot(s, label=lab)\n",
    "#_ = plt.legend(loc=0)\n",
    "#f, axs = plt.subplots(1,3, figsize=(12,4))\n",
    "\n",
    "#axs[0].plot(sample_df['model.likelihood.variance'],\n",
    "#            sample_df['model.kern.variance'], 'k.', alpha = 0.15)\n",
    "#axs[0].set_xlabel('noise_variance')\n",
    "#axs[0].set_ylabel('signal_variance')\n",
    "\n",
    "#axs[1].plot(sample_df['model.likelihood.variance'],\n",
    "#            sample_df['model.kern.lengthscales'], 'k.', alpha = 0.15)\n",
    "#axs[1].set_xlabel('noise_variance')\n",
    "#axs[1].set_ylabel('lengthscale')\n",
    "\n",
    "#axs[2].plot(sample_df['model.kern.lengthscales'],\n",
    "#            sample_df['model.kern.variance'], 'k.', alpha = 0.1)\n",
    "#axs[2].set_xlabel('lengthscale')\n",
    "#axs[2].set_ylabel('signal_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
    "plt = matplotlib.pyplot\n",
    "\n",
    "X = np.linspace(-3,3,20)\n",
    "Y = np.random.exponential(np.sin(X)**2)\n",
    "\n",
    "with gpflow.defer_build():\n",
    "    k = gpflow.kernels.Matern32(1, ARD=False) + gpflow.kernels.Bias(1)\n",
    "    l = gpflow.likelihoods.Exponential()\n",
    "    m = gpflow.models.GPMC(X[:,None], Y[:,None], k, l)\n",
    "\n",
    "m.kern.kernels[0].lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kern.kernels[1].lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kernels.Bias.variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "#m.kernels.Matern32.lengthscales.prior = gpflow.priors.Gamma(1., 1.)\n",
    "#m.kernels.Matern32.variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "#m.kernels.Bias.variance.prior = gpflow.priors.Gamma(1.,1.)\n",
    "\n",
    "m.compile()\n",
    "o = gpflow.train.AdamOptimizer(0.01)\n",
    "o.minimize(m, maxiter=15) # start near MAP\n",
    "\n",
    "s = gpflow.train.HMC()\n",
    "samples = s.sample(m, 100, epsilon=0.12, lmax=20, lmin=5, thin=5, logprobs=False)#, verbose=True)\n",
    "samples.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(samples.iloc[:,1]))\n",
    "print(np.var(samples.iloc[:,1]))\n",
    "print(np.mean(samples.iloc[:,2]))\n",
    "print(np.var(samples.iloc[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a more informative plot\n",
    "#plt.figure(figsize=(16, 4))\n",
    "#for lab, s in samples.iteritems():\n",
    "#    plt.plot(s, label=lab)\n",
    "#_ = plt.legend(loc=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for col in samples.columns.sort_values()[1:]:\n",
    "#    samples[col].hist(label=col.split('.')[-1], alpha=0.4, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.linspace(-4,4,100)[:,None]\n",
    "f_samples = []\n",
    "for i, s in samples.iterrows():\n",
    "    m.assign(s)\n",
    "    f_samples.append(m.predict_f_samples(xtest, 5, initialize=False))\n",
    "f_samples = np.vstack(f_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_samples = np.exp(f_samples[:, :, 0])\n",
    "\n",
    "line, = plt.plot(xtest, np.mean(rate_samples, 0), lw=2)\n",
    "plt.fill_between(xtest[:,0],\n",
    "                 np.percentile(rate_samples, 5, axis=0),\n",
    "                 np.percentile(rate_samples, 95, axis=0),\n",
    "                 color=line.get_color(), alpha = 0.2)\n",
    "\n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "plt.ylim(-0.1, np.max(np.percentile(rate_samples, 95, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(N,1)*0.1 + 3\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "\n",
    "k = gpflow.kernels.Matern52(1, lengthscales=0.3)\n",
    "m = gpflow.models.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = 0.01\n",
    "m.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(m):\n",
    "    xx = np.linspace(-0.1, 1.1, 100)[:,None]\n",
    "    mean, var = m.predict_y(xx)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2)\n",
    "    plt.plot(xx, mean, 'b', lw=2)\n",
    "    plt.fill_between(xx[:,0], mean[:,0] - 2*np.sqrt(var[:,0]), mean[:,0] + 2*np.sqrt(var[:,0]), color='blue', alpha=0.2)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as d\n",
    "\n",
    "print(X_train[0:10,:])\n",
    "\n",
    "space_length_scale = 1.0\n",
    "time_length_scale = 1.0\n",
    "\n",
    "#X_space = X_train[:,0:2]\n",
    "\n",
    "X_space = np.atleast_2d(X_train[0,0:2])\n",
    "Y_space = np.atleast_2d(X_train[1,0:2])\n",
    "X_time = np.atleast_2d(X_train[0,2])\n",
    "Y_time = np.atleast_2d(X_train[1,2])\n",
    "print(X_space[0:5,:])\n",
    "print(X_space.shape)\n",
    "print(X_time[0:5,:])\n",
    "print(X_time.shape)\n",
    "\n",
    "\n",
    "dists_space = d.cdist(X_space / space_length_scale, Y_space / space_length_scale, metric='sqeuclidean')\n",
    "dists_time = d.cdist(X_time / time_length_scale, Y_time / time_length_scale, metric='sqeuclidean')\n",
    "K = np.exp(-.5 * dists_space) * np.exp(-.5 * dists_time)\n",
    "\n",
    "print(dists_space)\n",
    "print(dists_time)\n",
    "print(K)\n",
    "\n",
    "\n",
    "dists_space = d.pdist(X_space / space_length_scale, metric='sqeuclidean')\n",
    "dists_time = d.pdist(X_time / time_length_scale, metric='sqeuclidean')\n",
    "K = np.exp(-.5 * dists_space) * np.exp(-.5 * dists_time)\n",
    "# convert from upper-triangular matrix to square matrix\n",
    "K = d.squareform(K)\n",
    "np.fill_diagonal(K, 1)\n",
    "\n",
    "print(dists_space)\n",
    "print(dists_time)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define custom kernel\n",
    "\n",
    "class RBF_space_RBF_time(StationaryKernelMixin, NormalizedKernelMixin, Kernel):\n",
    "    def __init__(self, space_length_scale=1.0, time_length_scale=1.0, space_length_scale_bounds=(1e-5, 1e5), time_length_scale_bounds=(1e-5, 1e5),):\n",
    "        self.space_length_scale = space_length_scale\n",
    "        self.time_length_scale = time_length_scale\n",
    "        self.space_length_scale_bounds = space_length_scale_bounds\n",
    "        self.time_length_scale_bounds = time_length_scale_bounds\n",
    "\n",
    "    #@property\n",
    "    #def anisotropic(self):\n",
    "    #    return np.iterable(self.length_scale) and len(self.length_scale) > 1\n",
    "\n",
    "    @property\n",
    "    def hyperparameter_space_length_scale(self):\n",
    "        #if self.anisotropic:\n",
    "        #    return Hyperparameter(\"length_scale\", \"numeric\",\n",
    "        #                          self.length_scale_bounds,\n",
    "        #                          len(self.length_scale))\n",
    "        return Hyperparameter(\n",
    "            \"space_length_scale\", \"numeric\", self.space_length_scale_bounds)\n",
    "    \n",
    "    @property\n",
    "    def hyperparameter_time_length_scale(self):\n",
    "        #if self.anisotropic:\n",
    "        #    return Hyperparameter(\"length_scale\", \"numeric\",\n",
    "        #                          self.length_scale_bounds,\n",
    "        #                          len(self.length_scale))\n",
    "        return Hyperparameter(\n",
    "            \"time_length_scale\", \"numeric\", self.time_length_scale_bounds)\n",
    "    \n",
    "    def __call__(self, X, Y=None, eval_gradient=False):\n",
    "        ## collect space coordinates (lat/long)\n",
    "        X_space = np.atleast_2d(X[:,0:2])\n",
    "        ## collect time coordinates (year)\n",
    "        X_time = np.atleast_2d(X[:,2])\n",
    "        #X_space = np.atleast_2d(X)\n",
    "        space_length_scale = self.space_length_scale\n",
    "        #length_scale = _check_length_scale(X, self.length_scale)\n",
    "        if Y is None:\n",
    "            dists_space = pdist(X_space / space_length_scale, metric='sqeuclidean')\n",
    "            dists_time = pdist(X_time / time_length_scale, metric='sqeuclidean')\n",
    "            K = np.exp(-.5 * dists_space) * np.exp(-.5 * dists_time)\n",
    "            # convert from upper-triangular matrix to square matrix\n",
    "            K = squareform(K)\n",
    "            np.fill_diagonal(K, 1)\n",
    "        else:\n",
    "        #    if eval_gradient:\n",
    "        #        raise ValueError(\n",
    "        #            \"Gradient can only be evaluated when Y is None.\")\n",
    "            Y_space = np.atleast_2d(Y[:,0:2])\n",
    "            Y_time = np.atleast_2d(Y[:,2])\n",
    "            dists_space = cdist(X_space / space_length_scale, Y_space / space_length_scale,\n",
    "                          metric='sqeuclidean')\n",
    "            dists_time = cdist(X_time / time_length_scale, Y_time / time_length_scale,\n",
    "                          metric='sqeuclidean')\n",
    "            K = np.exp(-.5 * dists_space) * np.exp(-.5 * dists_time)\n",
    "        \n",
    "        return K\n",
    "\n",
    "        #if eval_gradient:\n",
    "        #    if self.hyperparameter_length_scale.fixed:\n",
    "        #        # Hyperparameter l kept fixed\n",
    "        #        return K, np.empty((X.shape[0], X.shape[0], 0))\n",
    "        #    elif not self.anisotropic or length_scale.shape[0] == 1:\n",
    "        #        K_gradient = \\\n",
    "        #            (K * squareform(dists))[:, :, np.newaxis]\n",
    "        #        return K, K_gradient\n",
    "        #    elif self.anisotropic:\n",
    "        #        # We need to recompute the pairwise dimension-wise distances\n",
    "        #        K_gradient = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 \\\n",
    "        #            / (length_scale ** 2)\n",
    "        #        K_gradient *= K[..., np.newaxis]\n",
    "        #        return K, K_gradient\n",
    "        #else:\n",
    "        #    return K\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{0}(length_scale={1:.3g})\".format(self.__class__.__name__, np.ravel(self.space_length_scale)[0])\n",
    "        #if self.anisotropic:\n",
    "        #    return \"{0}(length_scale=[{1}])\".format(\n",
    "        #        self.__class__.__name__, \", \".join(map(\"{0:.3g}\".format,\n",
    "        #                                           self.length_scale)))\n",
    "        #else:  # isotropic\n",
    "        #    return \"{0}(length_scale={1:.3g})\".format(\n",
    "        #        self.__class__.__name__, np.ravel(self.length_scale)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ?? Should I specify a constant kernel?\n",
    "kernel = RBF(length_scale=1)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1, normalize_y=True, n_restarts_optimizer=5)\n",
    "all_accuracies = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2').mean()\n",
    "print(all_accuracies)\n",
    "## winter data\n",
    "## sigma = 1000, alpha=1\n",
    "## 0.8037412498178955\n",
    "## sigma = 1, alpha=1\n",
    "## 0.8055139240920436\n",
    "## sigma=1, alpha = 100\n",
    "## 0.14285243700525588\n",
    "## sigma=10, alpha=100\n",
    "## 0.14285243655664298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best hyperparameters: sigma\n",
    "all_accuracies_mean = []\n",
    "all_accuracies_std = []\n",
    "sigmas = np.arange(start=0.1, stop=4.5, step=0.5)\n",
    "for sigma in sigmas:\n",
    "    kernel = RBF(length_scale=sigma)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1, normalize_y=True, n_restarts_optimizer=5)\n",
    "    all_accuracies = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2')\n",
    "    all_accuracies_mean.append(all_accuracies.mean())\n",
    "    all_accuracies_std.append(all_accuracies.std())\n",
    "print(sigmas)\n",
    "print(all_accuracies_mean)\n",
    "print(all_accuracies_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to select the best parameter\n",
    "def best_parameter (mean_acc, param_list):\n",
    "    ## take sd of all accuracies, if small, then take the median sigma\n",
    "    acc_sd = np.std(mean_acc)\n",
    "    if acc_sd < 0.005:\n",
    "        best_param = np.median(param_list)\n",
    "    else:\n",
    "        ## pick the sigma with the highest R^2\n",
    "        best_param = param_list[np.argmax(mean_acc)]\n",
    "    \n",
    "        ## if large sd and tie, default to larger sigma (more variance)\n",
    "        ## or could select the default value (1)\n",
    "        #best_param = 1\n",
    "\n",
    "    return(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find best parameter for sigma\n",
    "best_sigma = best_parameter(all_accuracies_mean, sigmas)\n",
    "print(best_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best hyperparameters: alpha\n",
    "all_accuracies_al_mean = []\n",
    "all_accuracies_al_std = []\n",
    "alphas = np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "for alpha in alphas:\n",
    "    kernel = RBF(length_scale=best_sigma)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha, normalize_y=True, n_restarts_optimizer=5)\n",
    "    all_accuracies_al = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2')\n",
    "    all_accuracies_al_mean.append(all_accuracies_al.mean())\n",
    "    all_accuracies_al_std.append(all_accuracies_al.std())\n",
    "print(alphas)\n",
    "print(all_accuracies_al_mean)\n",
    "print(all_accuracies_al_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the best alpha\n",
    "best_alpha = best_parameter(all_accuracies_al_mean, alphas)\n",
    "print(best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now train model with best parameters\n",
    "kernel = RBF(length_scale=best_sigma)\n",
    "## ?? Should I re-run CV again when getting the final thetas?\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=best_alpha, normalize_y=True, n_restarts_optimizer=5)\n",
    "gp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigma = gp.predict(X_val, return_std=True)\n",
    "print(gp.score(X_val, y_val))\n",
    "print(y_pred[0:10])\n",
    "\n",
    "## save model object\n",
    "import pickle\n",
    "filename = 'finalized_model_base_jan.sav'\n",
    "pickle.dump(gp, open(filename, 'wb'))\n",
    "\n",
    "## how to load back in later to compare\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function, the prediction and the 95% confidence interval based on\n",
    "# the MSE\n",
    "plt.figure()\n",
    "#plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')\n",
    "## ?? How to visualize X values?\n",
    "plt.plot(X_val, y_pred, 'b-', label=u'Prediction')\n",
    "#plt.fill(np.concatenate([x, x[::-1]]),\n",
    "#         np.concatenate([y_pred - 1.9600 * sigma,\n",
    "#                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "#         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "#plt.xlabel('$x$')\n",
    "#plt.ylabel('$f(x)$')\n",
    "#plt.ylim(-10, 20)\n",
    "#plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  First the noiseless case\n",
    "X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n",
    "\n",
    "# Observations\n",
    "y = f(X).ravel()\n",
    "\n",
    "# Mesh the input space for evaluations of the real function, the prediction and\n",
    "# its MSE\n",
    "x = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
    "\n",
    "# Instantiate a Gaussian Process model\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(X, y)\n",
    "\n",
    "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "y_pred, sigma = gp.predict(x, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
