{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in relevant packages\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as pl\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##===================================================================================\n",
    "## read in data\n",
    "\n",
    "## winter\n",
    "## collapsed over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_winter_collapsed.csv\", sep=',') \n",
    "## aggregated over time\n",
    "dta = pd.read_csv(\"../data/kcl_london_model_data_winter_agg_time.csv\", sep=',')\n",
    "\n",
    "## not winter\n",
    "## collapsed over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_nowinter_collapsed.csv\", sep=',')\n",
    "## aggregated over time\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_nowinter_agg_time.csv\", sep=',')\n",
    "\n",
    "## monthly data (2000-2019)\n",
    "#dta = pd.read_csv(\"../data/kcl_london_model_data_monthly.csv\", sep=',')\n",
    "##==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            site code   latitude  longitude   site_type  year  \\\n",
      "0               Heathrow Airport  LH2  51.479234  -0.440531  Industrial  2000   \n",
      "1       Barnet - Tally Ho Corner  BN1  51.614675  -0.176607    Kerbside  2000   \n",
      "2         Camden - Swiss Cottage  CD1  51.544219  -0.175284    Kerbside  2000   \n",
      "3  Westminster - Marylebone Road  MY1  51.522540  -0.154590    Kerbside  2000   \n",
      "4              Croydon - Norbury  CR5  51.411349  -0.123110    Kerbside  2000   \n",
      "\n",
      "          nox  \n",
      "0  148.090848  \n",
      "1  199.026427  \n",
      "2  217.054604  \n",
      "3  457.892734  \n",
      "4  235.985324  \n",
      "(1981, 7)\n",
      "(1981, 2)\n",
      "(1981,)\n",
      "[148.09084761 199.0264266  217.05460423 457.89273426 235.98532394\n",
      " 433.92134367 239.16108482 167.38472428 184.71412974 161.47389498]\n",
      "[[51.614675   -0.176607  ]\n",
      " [51.544219   -0.175284  ]\n",
      " [51.52254    -0.15459   ]\n",
      " [51.411349   -0.12311   ]\n",
      " [51.55834619  0.06999151]\n",
      " [51.593935    0.085516  ]\n",
      " [51.552264   -0.402779  ]\n",
      " [51.51074135 -0.37162352]\n",
      " [51.489321   -0.310002  ]]\n"
     ]
    }
   ],
   "source": [
    "## preview data\n",
    "print(dta.head())\n",
    "print(dta.shape)\n",
    "\n",
    "## divide into features and variable\n",
    "X = dta[['latitude', 'longitude']].values  \n",
    "y = dta.loc[:,'nox'].values  \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "## print previews\n",
    "print(y[0:10])\n",
    "print(X[1:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485, 2)\n",
      "(1485,)\n",
      "(496, 2)\n",
      "(496,)\n"
     ]
    }
   ],
   "source": [
    "## create validation dataset (no test set since using CV)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485, 2)\n",
      "[[ 1.16995822  1.93651833]\n",
      " [ 1.42255373 -1.51986064]\n",
      " [ 0.03918873 -0.82917594]\n",
      " [-2.09211187 -1.06546145]\n",
      " [-2.00818931  0.06906187]\n",
      " [-2.00818931  0.06906187]\n",
      " [ 0.20401868 -0.17958956]\n",
      " [ 0.28445892  0.02733805]\n",
      " [ 1.62651214 -0.30395794]]\n",
      "[[ 0.69952956 -0.32300044]\n",
      " [ 1.89548528 -1.11041401]\n",
      " [ 0.23024601 -0.01530015]\n",
      " [ 1.16995822  1.93651833]\n",
      " [-0.36526168 -1.53115304]\n",
      " [-0.20738336 -2.02127878]\n",
      " [-0.19727365 -1.18250426]\n",
      " [-0.73650062 -0.42421379]\n",
      " [-2.22706774 -0.26339495]]\n"
     ]
    }
   ],
   "source": [
    "#y_train = normalize(y_train[:,np.newaxis], axis=0).ravel()\n",
    "#y_val = normalize(y_val[:,np.newaxis], axis=0).ravel()\n",
    "\n",
    "## should I standardize X-values??\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "feature_scaler = StandardScaler()  \n",
    "X_train = feature_scaler.fit_transform(X_train)  \n",
    "X_val = feature_scaler.transform(X_val)  \n",
    "print(X_train[1:5,:])\n",
    "print(X_val[1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18000194568260935\n"
     ]
    }
   ],
   "source": [
    "kernel_0 = RBF(1000, (1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel_0, normalize_y=True)\n",
    "all_accuracies = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2').mean()\n",
    "print(all_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best hyperparameters: sigma\n",
    "all_accuracies_mean = []\n",
    "all_accuracies_std = []\n",
    "sigmas = np.arange(start=0.1, stop=4.5, step=0.5)\n",
    "for sigma in sigmas:\n",
    "    kernel = RBF(length_scale=sigma)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=1, n_restarts_optimizer=5)\n",
    "    all_accuracies = cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2')\n",
    "    all_accuracies_mean.append(all_accuracies.mean())\n",
    "    all_accuracies_std.append(all_accuracies.std())\n",
    "print(sigmas)\n",
    "print(all_accuracies_mean)\n",
    "print(all_accuracies_std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search for best hyperparameters: alpha\n",
    "all_accuracies_al = []\n",
    "alphas = np.arange(start=0.1, stop=2, step=0.1)\n",
    "for alpha in alphas:\n",
    "    kernel = RBF(length_scale=best_sigma)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha, n_restarts_optimizer=5)\n",
    "    all_accuracies_al.append(cross_val_score(estimator=gp, X=X_train, y=y_train, cv=5, scoring='r2'))\n",
    "print(alphas)\n",
    "print(all_accuracies_al.mean())\n",
    "print(all_accuracies_al.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigma = gp.predict(x, return_std=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
